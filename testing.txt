START:
sudo systemctl start kafka

/opt/spark/sbin/start-all.sh

CREATE TOPIC:
/usr/local/kafka/bin/kafka-topics.sh --create --topic reddit-raw-posts --bootstrap-server localhost:9092 --partitions 1 --replication-factor 1
/opt/kafka/bin/kafka-topics.sh --create --topic reddit-keyword-filtered --bootstrap-server localhost:9092 --partitions 1 --replication-factor 1

POSTGRES:
psql -U root -W -d reddit_stream_db -h localhost
Password: 
psql (14.17 (Ubuntu 14.17-0ubuntu0.22.04.1))
SSL connection (protocol: TLSv1.3, cipher: TLS_AES_256_GCM_SHA384, bits: 256, compression: off)
Type "help" for help.

-- Drop all existing tables
DO
$$
DECLARE
    r RECORD;
BEGIN
    FOR r IN (SELECT tablename FROM pg_tables WHERE schemaname = 'public') LOOP
        EXECUTE 'DROP TABLE IF EXISTS ' || quote_ident(r.tablename) || ' CASCADE';
    END LOOP;
END;
$$;

-- Table 1: Sentiment analysis results (with full post)
CREATE TABLE sentiment_results (
    id TEXT PRIMARY KEY,
    title TEXT,
    selftext TEXT,
    score INTEGER,
    created_utc TIMESTAMP,
    num_comments INTEGER,
    sentiment TEXT
);

-- Table 2: Raw filtered posts without sentiment
CREATE TABLE reddit_keyword_filtered (
    id TEXT PRIMARY KEY,
    title TEXT,
    selftext TEXT,
    score INTEGER,
    created_utc TIMESTAMP,
    num_comments INTEGER
);

-- Table 3: Title-only sentiment (optional use-case)
CREATE TABLE batch_title_sentiment (
    id TEXT PRIMARY KEY,
    title TEXT,
    title_sentiment TEXT
);


RUNNING:
python3 ingestion/reddit_producer.py

CHECKING:
python3 ingestion/cons.py

SPARK:
 /home/sravani/.local/bin/spark-submit     --jars /home/sravani/jars/postgresql-42.3.1.jar     --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.1     spark_streaming/spark_sentiment_consumer.py

 fix jar file location in command and in code to local one


 CHECKING POSTGRES:
reddit_stream_db=> \x
Expanded display is on.
reddit_stream_db=> SELECT * FROM sentiment_results ORDER BY created_utc DESC LIMIT 10;


 /home/sravani/.local/bin/spark-submit --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.2.1   /home/sravani/dbt/DBT-PROJECT/spark_streaming/keyword_filter.py


 /home/sravani/.local/bin/spark-submit   --jars /home/sravani/jars/postgresql-42.3.1.jar   --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0   /home/sravani/dbt/DBT-PROJECT/spark_streaming/spark_sentiment_consumer.py

if spark/kafka in opt folder use this command:

/opt/spark/bin/spark-submit \
    --jars /opt/jars/postgresql-42.3.1.jar \
    --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.1 \
    /root/Reddit-Stream/DBT-PROJECT/batch_processing/batch_title_sentiment.py


 /home/sravani/.local/bin/spark-submit   --jars /home/sravani/jars/postgresql-42.3.1.jar   --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.2.1   /home/sravani/dbt/DBT-PROJECT/spark_streaming/hot_topic_aggregator.py


 CREATE TABLE trending_words_batch (
    batch_id INT,
    word VARCHAR(255),
    count INT,
    window_start TIMESTAMP,
    window_end TIMESTAMP,
    PRIMARY KEY (batch_id, word)
);


reddit_stream_db=> CREATE TABLE sentiment_aggregated_batch (
    window_start TIMESTAMP,
    window_end TIMESTAMP,
    positive_count INTEGER,
    negative_count INTEGER,
    neutral_count INTEGER
);

reddit_stream_db=> CREATE TABLE sentiment_aggregated (
    window_start TIMESTAMP,
    window_end TIMESTAMP,
    positive_count INTEGER,
    negative_count INTEGER,
    neutral_count INTEGER
);


ALTER TABLE sentiment_results
ADD COLUMN ingestion_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP;

 /home/sravani/.local/bin/spark-submit   --jars /home/sravani/jars/postgresql-42.3.1.jar   --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0   /home/sravani/dbt/DBT-PROJECT/spark_streaming/sentiment_aggregator.py

TRUNCATE TABLE public.reddit_keyword_filtered RESTART IDENTITY CASCADE;
TRUNCATE TABLE public.sentiment_aggregated RESTART IDENTITY CASCADE;
TRUNCATE TABLE public.sentiment_aggregated_batch RESTART IDENTITY CASCADE;
TRUNCATE TABLE public.sentiment_results RESTART IDENTITY CASCADE;
TRUNCATE TABLE public.trending_words_batch RESTART IDENTITY CASCADE;
